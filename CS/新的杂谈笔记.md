# 书单
符号逻辑学：语法，语义和证明

算法之美

Dos命令行在Windows操作中的典型应用

P，NP,NP-complete确定非确定多项式时间可解

形式语言与自动机理论引论

可计算性理论

代数不变量的源流

Javascripts函数式编程



量子计算

量子算法








# course=class+homework+project(lab)+quiz(not graded)+exam


[为什么能行可计算的就是图灵可计算的（递归的）？
已知图灵可计算的（Turing computable）函数当且仅当是递归（recursive）函数。为什么一个函数是能行可计算的，当且仅当它是图灵可计算的或递归的？https://www.zhihu.com/question/267184153/answer/320101790](https://www.zhihu.com/question/267184153/answer/320101790)


**[https://zhuanlan.zhihu.com/p/35729117](https://zhuanlan.zhihu.com/p/35729117)**
# 计算机的数学思想源头（回复“计算机数学”可下载PDF典藏版）

[https://zhuanlan.zhihu.com/p/51156918](https://zhuanlan.zhihu.com/p/51156918)

循环算法也都可以用递归算法解决，递归算法对比比循环算法，优势在于递归算法逻辑简单，劣势就是空间复杂度比较高。递归中有一种特殊的递归叫尾递归，所有递归都可以转化为尾递归，如果计算机科学解决编译器的尾递归内存复用问题，那么这将是计算机编程界的一次变革，很多算法问题都可以变得很简单，并且不损失运行效率。

以数据库系统来说：从上层的数据模式（Schema）及演算算式，到信息世界的数据模型（抽象数据结构），再到底层的数据管理及其存储，经过了数学、软件抽象、硬件实现这几层。可以看出数据库系统是一个完整的计算机系统。

再看多媒体系统：从上层的数据模式（图像矩阵、音视频序列）及其演算算式，到信息世界的数据模型，再到底层的的数据管理及其存储，也是经过了数学、软件抽象、硬件实现。

在这个过程中，各有自己的一套完整的符号和范式系统。科班出身的优势在于，他受到这样的一种完整的熏陶和培养。非科班出身的人，没有受过这种符号和范式的熏陶。虽然看起来，大学四年没有学到什么东西，但是这种潜移默化不容易被替代。除非这个人思想非常成熟，能够根据既有经验自行脑补。

  
  
作者：tom pareto  
链接：https://www.zhihu.com/question/57746751/answer/155859310  
来源：知乎  
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。











计算机组成原理→DOS命令→汇编语言→C语言（不包括C++）、代码书写规范→数据结构、编译原理、操作系统→计算机网络、数据库原理、正则表达式→其它语言（包括C++）、架构……

VC调试(TC或BC用TD调试)时按Alt+8、Alt+6和Alt+5,打开汇编窗口、内存窗口和寄存器窗口看每句C对应的汇编、单步执行并观察相应内存和寄存器变化，这样过一遍不就啥都明白了吗。
对VC来说，所谓‘调试时’就是编译连接通过以后，按F10或F11键单步执行一步以后的时候，或者在某行按F9设了断点后按F5执行停在该断点处的时候。
（Linux或Unix下可以在用GDB调试时,看每句C对应的汇编并单步执行观察相应内存和寄存器变化。）

提醒：
“学习用汇编语言写程序”
和
“VC调试(TC或BC用TD调试)时按Alt+8、Alt+6和Alt+5,打开汇编窗口、内存窗口和寄存器窗口看每句C对应的汇编、单步执行并观察相应内存和寄存器变化，这样过一遍不就啥都明白了吗。
（Linux或Unix下可以在用GDB调试时,看每句C对应的汇编并单步执行观察相应内存和寄存器变化。）”
不是一回事！
https://bbs.csdn.net/topics/390102540
不要迷信书、考题、老师、回帖；
要迷信CPU、编译器、调试器、运行结果。
并请结合“盲人摸太阳”和“驾船出海时一定只带一个指南针。”加以理解。
任何理论、权威、传说、真理、标准、解释、想象、知识……都比不上摆在眼前的事实！

对学习编程者的忠告：
眼过千遍不如手过一遍！
书看千行不如手敲一行！
手敲千行不如单步一行！
单步源代码千行不如单步对应汇编一行！





链接：https://www.zhihu.com/question/270998611/answer/360930889
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
![](_v_images/1549851831_32534.png)
我当年自学计算机啃了好几个月的 C 然后发现还不如找一门简单的语言把计算机先入门了来得轻松。很多学校里面的课程都会让你花一个学期的时间学 C，然后就觉得你啥都懂了，就开始教各种数据结构、算法、操作系统原理、编译原理等。最后导致的问题是：你都不知道你学的这个是个啥。说白了就是：你如果没有办法把正在学的东西和你已经掌握的知识结合起来，你就很难明白它在说什么。我当年自学踩过无数的坑，最后总结的学习方法是：自顶向下学习法，首先学习你平时接触最多的东西，然后带着疑问去学习它背后的实现和逻辑。先学 Linux 基本操作，譬如怎么在电脑上装一台 Linux 出来、怎么查看进程、怎么操作文件、软链硬链、格式化磁盘、配置网卡、curl 一个网页等。你可能会问为啥要用 Linux 呢，因为你工作之后大部分公司的线上环境都是运行在 Linux 下，所以如果你打算做前端、后端或者系统开发，你必须具备 Linux 的操作能力。当年我看的是鸟哥，可以不用全部看完，当然现在应该很多这类书，可以自己找一下，知乎也有对应的回答 有没有比《鸟哥的Linux私房菜》更好的书？ 目的都一样，先把 Linux 基本操作掌握了。强烈建议如果你希望自己的操作系统方面的知识好一些，赶紧把你的 Windows 系统装成 Ubuntu，或者换台 Mac 也行。当你 Linux 掌握的不错的时候，用你的 Linux 环境去学一门简单的编程语言，譬如 Python，当然你在学校里面学了 C 和 Java 也可以。我当年看的是 《A Byte of Python》 中文英文都有，知乎也有很多相关的推荐 你是如何自学 Python 的？ 记住一定要在你的 Linux 或者 Mac 环境下来学习，此时你应该能够做一些简单的编程，同时见过一些基本概念，譬如单进程程序、多线程并发处理请求等等。你可能不是很理解里面的一些概念，但是没有关系，你学完后面的之后，你就能彻底明白了。此时你可以开始学操作系统相关的概念了，《Linux内核设计与实现》 是一本超级良心的介绍操作系统的书，用短短 300 页把你之前所有的困惑全部梳理一遍。它不会讲的特别细，但是它会把你带入门。之后你可以找一本介绍计算机组成原理的书了，同样不会太厚，但是能带你先入门。《编码》 这本书就不错，400 页能告诉你为什么机器语言是二进制，CPU 是怎么实现等等。上面这两本书虽然不是很厚，但是已经非常良心的删减了很多不是必须知道的细节，可能需要反复看多几次才能理解，此时你会发现，原来我之前写错代码的原因是这个。算法、数据结构、数据库等在这里不介绍了。此时，你应该已经入门了操作系统，同时具备基本的编程能力，你如果真把 3，4 点提到的书都看懂了，大学毕业后找到一个好工作应该不难（很多在职的工程师其实很多这些里面的概念都不懂），之后就是看你自己发展方向，譬如 APUE、UNP、深入理解计算机系统都是好书，我觉得那个时候你应该能自己具备能力去寻找适合自己的书了。


作者：兔子老大
链接：https://www.zhihu.com/question/51499031/answer/126123021
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

前一段时间某个大数据比赛，因为智商不行只能洗数据和刷特征，有一次因改变了脚本数据结构(以及相对应的算法)，把原本O(n^2)，优化成O(n),对于需要频繁尝试不同的特征来验证模型的需求说，显然我加快了进度。而对于其他偏向底层和实时性的应用来说，数据结构和算法更为重要。操作系统，这门课你可千万不要觉得，“我又不需要写一个Linux，写一个windows所以我就不需要学习”。这门课不是叫你写一个操作系统的，讲的是资源的调度和分配，以及大型软件的设计。稍微写过一些并行并行的程序，都能体会到这门课的重要性。计算机网络，是不是觉得现在网络应用框架这么多，socket不用手写，物理层和链路层就更加不用理了是吧。最近尝试把一个网络应用部署到云服务器上，网络没问题，端口全开，云主机上localhost可访问，但从公网死活访问不了，至于原因就等题主你学习计算机网络的时候来思考一下吧。计算机组成原理，你还真不需要理解cpu的每一个电路怎么运行，但当我懂得内存与外存的区别，以及缓冲区这些概念时，代码的书写思维和我入门时真的有很大不同。所有的语言，框架说白了就是在某种规则(语法规则，API)上进行利用(编写代码)，而一旦遇上瓶颈，或者某些bug的定位，需要为某种环境而进行方案的选择折衷（比如存储空间），这个时候就是你说的那些基础知识辅助决策的时候了。当然了，后面说的问题可能有些程序员很少遇到甚至未遇到过，可是一旦遇到了，你真的有信心马上现学现用吗？说不定这个瓶颈就是一个升职机会呢？你要不要？如果你真的打算在技术这条路上走下去，听楼上金老师的话，好好打好基础吧……


作者：Raynor  
链接：https://www.zhihu.com/question/22017267/answer/26468016  
来源：知乎  
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。  
  

作为多年的老码畜，我大概简单的，至下而上回答一下这个问题好了：

**计算机组成原理**  
这门学科告诉你什么是计算机。  
首先，我们可以把计算机分解成最原始的部件——晶体管。晶体管是一种半导体材料，其最重要的作用就是半导：可以通过电流的变化，实现电路的切换。比如计算机最基础的与或非运算，都可以通过晶体管组成的电子元件实现。而通过晶体管的电位差不同，就可以体现"二进制数据"，即0和1。再加上电容和电阻，就能把这种二进制数据临时保存起来。  
综合这些特性，大牛们发现把晶体管用作精密的数学计算，可以极大的提高运算的效率。比如我有2个电容，分别是充满电和没有电，对他们同时释放电信号，电容就会把其中的电子放出来，经过特定的逻辑电路，如与门，得到了0的结果。要计算1+1，实际上也是类似的原理。我先设计一个加法电路，把若干电容组合成的"数字"流过这个电路，把结果存入目标电容，就得到了结果。大规模的复杂运算以此类推。  
最早期的计算机真的就是用许多结晶体管实现的复杂电路结构，通过控制输入电流得到希望的输出结果。后来人们发现，这种计算可以用某些形式抽象成多种指令，不用针对每次计算设计复杂的电路，只要调用指令就可以实现任何一种计算组合，于是诞生了cpu。只有cpu，每次都要自己配置输入信号，实在太痛苦，就做了纸带输入给计算机。后来又发现纸带还是很麻烦，于是发明了输入终端和对应的存储设备。后来又发现很多数据要临时保存起来，供连续计算使用，于是发明了内存。再后来pc的发展经历了无数次的变革，让计算机一步步到了今天的地步，也就是你现在看到的这样。  
其中的历程非常曲折，也许有机构能够把他们全部组织成一本漫长的历史，但个人肯定是无能为力的。

  

**操作系统**  
综上所述，计算机发展到一定程度，什么东西都靠人工也未免太累了。  
比如通过输入设备组织指令给cpu去计算，你希望能够找一个快速的输入设备(比如键盘)，在能看到结果的地方输入(比如屏幕)，然后再用很方便的方式提交给cpu(比如按键或者指令)，让cpu去算好了，再把结果展示出来(比如屏幕)。  
理想很美好，但是这么复杂的流程，人工管理起来不还是很麻烦吗？除非我构造一个设备，把这些所有设备都管理起来，于是主板就诞生了。

现在主板解决了我们大量的问题，但是我发觉我的需求还远远不够！  
我希望我写过的程序能在任何一台机上运行。  
我希望我能边听音乐边干活——即同一时间可以运行多个程序。  
我希望别人写的傻×东西不要影响到我的工作——即多任务控制。  
我希望计算机里面的各种资源都能得到良好的组织，更快的访问。  
我希望我的用户界面更好看，使用更方便，功能更强大！  
我是个小白用户，啥都不懂，别跟我扯这些有的没的，我就像随便操作两下就能达到我想要的！  
如果这些需求全部都做在主板bios里面，那将是一场灾难！除非bios经过极大的调整和改动，划分出一大块区域存放操作系统，并且完成复杂的体系结构改革。  
计算机发展到这种程度，早就已经有很多的机构和厂商介入其中，试图从中渔利。他们当然不会求着计算机标准委员会和主板生产厂商去做所谓的主板改革，而是编写自己的程序——操作系统，来解决这些所有的问题。

而操作系统问世之后，一方面接管了主板对于系统资源的管理，加入了自己的中间层——驱动程序，另一方面又充分发挥了人机交互的接口——gui界面，成为了计算机必不可少的组成部分。  
操作系统通过bios引导，即作为应用程序开始运行。我们知道程序的本质上就是在cpu上运行种种指令，比如操作系统需要把硬盘上的模块放入内存，实际上就是运行了一系列复杂的cpu指令，cpu指令通过主板bus（实际上就是传递指令的电路）发送指令给硬盘（比如从哪个扇区偏移多少读多少数据），硬盘再通过芯片组转动磁头，把数据读到缓存中，完成后给cpu发送一个信号（即中断），cpu收到这个信号，就在寄存器中寻址该信号对应的地址（即我们说的中断向量表），运行该地址中的指令，发现该指令是发送拷贝指令给主板芯片组，主板就会在cpu的指导下不断的发送信号，告诉硬盘缓存放电，再把接收的电信号存到指定的内存位置去，如此反复，直到完成cpu的一系列指令为止。  
操作系统说白了，就是这样通过种种cpu指令，实现自身的所有功能。  
当然这些指令也不是一条条写进去的，而是通过编程语言完成人类较容易识别的逻辑，然后再通过编译器把这些逻辑翻译成cpu指令，这就涉及编译原理的东西了。

既然操作系统对硬件的访问都是通过cpu指令来完成的，那为什么大家都感觉是操作掌管了硬件呢？这就涉及操作系统最本质的功能之一：对系统资源的管控了。  
我们运行的所有程序，实际上都是操作系统帮我们运行的。操作系统背后进行了很多的工作，如虚拟地址空间的分配，cpu分时调度，硬件中断信号的响应等。这样对于硬件资源的访问，也是通过操作系统安排的。比如操作系统会通过把短时间内硬盘读写合并成顺序的方式，以提高磁头的利用率，降低磁头转向的时间。再比如对内存地址的访问也是由操作系统管控的，某个程序中的内存地址具体落到内存条的哪个位置，还是硬盘中的虚拟内存，就看操作系统的心情了。

至此，操作系统和硬件的交互也介绍的差不多了，更详细的东西建议参考操作系统相关的书籍吧，比如《深入理解计算机系统》，《linux内核设计与实现》，《unix环境高级编程》之类的。

  

**数据结构**  
数据结构的作用，就是为了提高硬件利用率。  
比如操作系统需要查找用户应用程序"office"在硬盘的哪个位置，盲目的搜索一遍硬盘肯定是低效的，这时候搞个b+树作为索引，搜索office这个单词就很快，然后就能很快的定位office这个应用程序的文件信息，再找到文件信息中对应的磁盘位置了。

数据结构的东西找本《算法导论》，《数据结构与算法分析》之类的看吧。

**计算机网络**  
计算机网络分为3块：  
1\. 硬件  
网卡，网线，交换机这些，用来处理数据的。  
2\. 协议  
数据在网络中通信如何组织？如何识别？如何保证数据的正确性？  
这2块我就不多说了。

3\. 操作系统  
这就是如何把计算机网络和操作系统结合起来的问题了。  
对于操作系统来说，网卡也是一种硬件资源。但是网络不单只是一种硬件，而是一种媒体入口。比如操作系统管理硬盘，当然不是简单的记一下硬盘有多大，然后一切操作都交给硬盘芯片去做，更多的需要组织硬盘的扇区，分区，记录文件和扇区/偏移的关系等等。  
操作系统对于网络来说也是如此，要记录自身在网络的标识（ip），可被他人访问的入口（port），以及对方的信息（remote ip/port）。连接，断开，数据确认等操作也是由协议控制。  
传递自身消息给对方，类似访问硬盘一样把内存中的数据传递给网卡缓存，再发消息给网卡让网卡去传数据，而是否发送成功这些保证不再由硬件中断信号反馈，而是通过网络协议完成。接收对方消息，也是接收到网卡中断，再把数据从网卡缓存移动到内存中，再通过协议给予对方反馈。

简单来说就是这样，如果以后有时间，我再把osi七层协议中的内容和操作系统的交互再详述一遍吧。

网络方面的就推荐《tcp/ip详解》，《uinx网络高级编程》吧。















